# [PT-BR] Pipeline de Engenharia de Dados: AnÃ¡lise de Vendas de E-commerce no Databricks

-----

## ðŸš€ VisÃ£o Geral do Projeto

Este projeto demonstra a construÃ§Ã£o de um pipeline de engenharia de dados **end-to-end** no **Databricks Community Edition**, focado na ingestÃ£o, limpeza, transformaÃ§Ã£o e agregaÃ§Ã£o de dados de vendas de e-commerce. A arquitetura segue o padrÃ£o **Data Lakehouse**, com camadas Bronze, Silver e Gold, utilizando **Apache Spark** e **Delta Lake** para garantir escalabilidade, confiabilidade e desempenho.

O objetivo principal Ã© transformar dados brutos de transaÃ§Ãµes de vendas em produtos de dados prontos para anÃ¡lise e consumo por Ã¡reas de negÃ³cio ou ferramentas de Business Intelligence (BI).

-----

## ðŸ’¡ Tecnologias Utilizadas

  * **Databricks Community Edition:** Plataforma unificada para engenharia de dados.
  * **Apache Spark:** Motor de processamento de dados distribuÃ­do e escalÃ¡vel.
  * **Delta Lake:** Formato de armazenamento de dados open-source que traz capacidades de ACID transactions, schema enforcement e time travel para data lakes.
  * **PySpark / Spark SQL:** APIs do Spark para manipulaÃ§Ã£o e transformaÃ§Ã£o de dados.
  * **Python:** Linguagem de programaÃ§Ã£o principal para os notebooks.
  * **Git / GitHub:** Controle de versÃ£o e colaboraÃ§Ã£o.

-----

## ðŸ—ï¸ Arquitetura do Pipeline (Data Lakehouse)

O pipeline Ã© estruturado em trÃªs camadas distintas, cada uma com um propÃ³sito especÃ­fico:

1.  **Bronze Layer (Raw Data):**

      * **PropÃ³sito:** IngestÃ£o de dados brutos da fonte (CSV) para o Data Lakehouse.
      * **CaracterÃ­sticas:** Dados quase intocados, mantendo o formato original, porÃ©m armazenados como tabelas Delta para durabilidade e rastreabilidade.
      * **Notebook:** `01_bronze_layer_ingestion.py`

2.  **Silver Layer (Cleaned & Conformed Data):**

      * **PropÃ³sito:** Limpeza, padronizaÃ§Ã£o e enriquecimento dos dados da camada Bronze.
      * **CaracterÃ­sticas:** Tratamento de valores nulos, remoÃ§Ã£o de duplicatas, correÃ§Ã£o de tipos de dados, filtragem de registros invÃ¡lidos e criaÃ§Ã£o de colunas derivadas (`total_price`).
      * **Notebook:** `02_silver_layer_transformation.py`

3.  **Gold Layer (Curated & Aggregated Data):**

      * **PropÃ³sito:** CriaÃ§Ã£o de produtos de dados agregados e otimizados para consumo analÃ­tico.
      * **CaracterÃ­sticas:** Dados sumarizados para anÃ¡lises de negÃ³cio, como vendas por produto, vendas diÃ¡rias e top clientes. Ideais para dashboards de BI.
      * **Notebook:** `03_gold_layer_product.py`

-----

## ðŸ“Š Produtos de Dados (Camada Gold)

Este pipeline gera as seguintes tabelas analÃ­ticas na camada Gold:

  * **`sales_by_product`:** Vendas totais (quantidade e receita) por cada item/produto.
  * **`daily_sales`:** Receita diÃ¡ria, contagem de transaÃ§Ãµes e nÃºmero de clientes Ãºnicos por dia.
  * **`top_customers`:** Lista dos clientes que mais gastaram, com o total da receita gerada e o nÃºmero de faturas.

-----

## âš™ï¸ Como Executar o Projeto

Para replicar e executar este pipeline no seu ambiente Databricks Community Edition:

### 1\. ConfiguraÃ§Ã£o do Databricks

  * **Crie uma Conta:** Acesse [Databricks Community Edition](https://community.cloud.databricks.com/) e crie sua conta gratuita.
  * **Crie um Cluster:**
      * No Databricks Workspace, vÃ¡ em **"Compute"**.
      * Clique em **"+ Create Cluster"**.
      * Nomeie o cluster (ex: `ecommerce-project-cluster`).
      * Escolha a versÃ£o mais recente do **Databricks Runtime (LTS)**.
      * O cluster serÃ¡ automaticamente encerrado apÃ³s um perÃ­odo de inatividade (padrÃ£o de 120 minutos) para economizar recursos da versÃ£o gratuita.

### 2\. ObtenÃ§Ã£o e Upload dos Dados

  * **Baixe o Dataset:** FaÃ§a o download do dataset "Online Retail" do Kaggle: [Online Retail Dataset](https://www.kaggle.com/datasets/carrie1/ecommerce-data). O arquivo Ã© `Online Retail.xlsx`.
  * **Converta para CSV:** Abra o arquivo `.xlsx` em um editor de planilhas (Excel, Google Sheets) e salve a primeira aba como `online_retail.csv`.
  * **FaÃ§a Upload para o DBFS/Volumes:**
      * No Databricks Workspace, vÃ¡ em **"Catalog"** (ou "Data").
      * Clique em **"Upload to Volume"**.
      * Selecione seu cluster.
      * **Destination path:** Crie um caminho para seus dados brutos. Recomenda-se `/Volumes/main/default/ecommerce_data/raw/` se a funcionalidade `Volumes` estiver disponÃ­vel. Caso contrÃ¡rio, use `dbfs:/FileStore/datasets/ecommerce_data/raw/`.
      * Arraste e solte o arquivo `online_retail.csv` ou clique para selecionÃ¡-lo e finalize o upload.

### 3\. ImportaÃ§Ã£o dos Notebooks

  * **Baixe os Notebooks:** Clone este repositÃ³rio para sua mÃ¡quina local ou baixe os arquivos `.py` da pasta `notebooks/`.
  * **Importe para o Databricks:**
      * No Databricks Workspace, vÃ¡ em **"Workspace"**.
      * Clique com o botÃ£o direito na sua pasta de usuÃ¡rio (ou em "Shared") e selecione **"Import"**.
      * Escolha **"File"** e faÃ§a upload de cada arquivo `.py` (ex: `01_bronze_layer_ingestion.py`, `02_silver_layer_transformation.py`, `03_gold_layer_product.py`). Certifique-se de que o **"Import as"** esteja configurado para **"Notebook"**.

### 4\. ExecuÃ§Ã£o do Pipeline

  * **Anexe ao Cluster:** Abra cada notebook e certifique-se de que ele esteja anexado ao cluster que vocÃª criou.
  * **Execute em Ordem:** Execute os notebooks na seguinte sequÃªncia:
    1.  `01_bronze_layer_ingestion.py`
    2.  `02_silver_layer_transformation.py`
    3.  `03_gold_layer_product.py`

Monitore a execuÃ§Ã£o de cada notebook e verifique as saÃ­das para garantir que o pipeline estÃ¡ funcionando corretamente. As tabelas Delta serÃ£o criadas no `dbfs:/user/hive/warehouse/ecommerce_data/` (ou caminho similar) e poderÃ£o ser consultadas via SQL ou PySpark.

-----

## ðŸ“ˆ VisualizaÃ§Ã£o e AnÃ¡lise

ApÃ³s a execuÃ§Ã£o do pipeline, as tabelas na camada Gold (`sales_by_product`, `daily_sales`, `top_customers`) estarÃ£o prontas para anÃ¡lise. VocÃª pode usar as capacidades de visualizaÃ§Ã£o embutidas no Databricks diretamente nas tabelas ou exportar os dados para ferramentas de BI externas (Power BI, Tableau, Looker) para criar dashboards interativos.

Exemplo de consulta no Databricks:

```sql
SELECT description, total_revenue
FROM sales_by_product
ORDER BY total_revenue DESC
LIMIT 20;
```

-----

## ðŸš€ PrÃ³ximos Passos e Melhorias

Este projeto pode ser expandido com as seguintes funcionalidades:

  * **SimulaÃ§Ã£o de Streaming com Auto Loader:** Implementar o Auto Loader no notebook da camada Bronze para simular a ingestÃ£o contÃ­nua de novos arquivos de vendas.
  * **ValidaÃ§Ã£o de Qualidade de Dados:** Adicionar mais verificaÃ§Ãµes de qualidade de dados na camada Silver (ex: validaÃ§Ãµes de consistÃªncia, detecÃ§Ã£o de anomalias).
  * **OtimizaÃ§Ã£o de Desempenho:** Explorar tÃ©cnicas como particionamento de tabelas Delta, Z-ordering e compactaÃ§Ã£o para otimizar o desempenho de consulta.
  * **IntegraÃ§Ã£o com Ferramentas de BI:** Conectar as tabelas Gold diretamente a ferramentas de BI para criar dashboards de vendas em tempo real.
  * **OrquestraÃ§Ã£o de Jobs:** Utilizar Databricks Workflows (Jobs) para agendar e orquestrar a execuÃ§Ã£o do pipeline automaticamente.

-----

**Autor:** Guilherme Noronha Mello -> github.com/guinnoronha
**Data:** Junho de 2025

-----


# [EN-US] Data Engineering Pipeline: E-commerce Sales Analysis on Databricks

-----

## ðŸš€ Project Overview

This project demonstrates the construction of an **end-to-end data engineering pipeline** on **Databricks Community Edition**, focusing on the ingestion, cleansing, transformation, and aggregation of e-commerce sales data. The architecture follows the **Data Lakehouse** pattern, with Bronze, Silver, and Gold layers, leveraging **Apache Spark** and **Delta Lake** to ensure scalability, reliability, and performance.

The primary goal is to transform raw sales transaction data into ready-to-use data products for business analysis or Business Intelligence (BI) tools.

-----

## ðŸ’¡ Technologies Used

  * **Databricks Community Edition:** Unified analytics platform for data engineering.
  * **Apache Spark:** Distributed and scalable data processing engine.
  * **Delta Lake:** Open-source storage format that brings ACID transactions, schema enforcement, and time travel capabilities to data lakes.
  * **PySpark / Spark SQL:** Spark APIs for data manipulation and transformation.
  * **Python:** Primary programming language for the notebooks.
  * **Git / GitHub:** Version control and collaboration.

-----

## ðŸ—ï¸ Pipeline Architecture (Data Lakehouse)

The pipeline is structured into three distinct layers, each serving a specific purpose:

1.  **Bronze Layer (Raw Data):**

      * **Purpose:** Ingest raw data from the source (CSV) into the Data Lakehouse.
      * **Characteristics:** Data remains largely untouched, maintaining its original format, but stored as Delta tables for durability and traceability.
      * **Notebook:** `01_bronze_layer_ingestion.py`

2.  **Silver Layer (Cleaned & Conformed Data):**

      * **Purpose:** Cleanse, standardize, and enrich the data from the Bronze layer.
      * **Characteristics:** Handles null values, removes duplicates, corrects data types, filters invalid records, and creates derived columns (`total_price`).
      * **Notebook:** `02_silver_layer_transformation.py`

3.  **Gold Layer (Curated & Aggregated Data):**

      * **Purpose:** Create aggregated data products optimized for analytical consumption.
      * **Characteristics:** Summarized data for business analysis, such as sales by product, daily sales, and top customers. Ideal for BI dashboards.
      * **Notebook:** `03_gold_layer_product.py`

-----

## ðŸ“Š Data Products (Gold Layer)

This pipeline generates the following analytical tables in the Gold layer:

  * **`sales_by_product`:** Total sales (quantity and revenue) for each item/product.
  * **`daily_sales`:** Daily revenue, transaction count, and unique customer count by day.
  * **`top_customers`:** A list of top-spending customers, including total revenue generated and the number of invoices.

-----

## âš™ï¸ How to Run the Project

To replicate and run this pipeline in your Databricks Community Edition environment:

### 1\. Databricks Setup

  * **Create an Account:** Go to [Databricks Community Edition](https://community.cloud.databricks.com/) and create your free account.
  * **Create a Cluster:**
      * In your Databricks Workspace, navigate to **"Compute"**.
      * Click on **"+ Create Cluster"**.
      * Name your cluster (e.g., `ecommerce-project-cluster`).
      * Choose the latest **Databricks Runtime (LTS)** version.
      * The cluster will automatically terminate after a period of inactivity (default 120 minutes) to conserve resources in the free version.

### 2\. Data Acquisition and Upload

  * **Download the Dataset:** Download the "Online Retail" dataset from Kaggle: [Online Retail Dataset](https://www.kaggle.com/datasets/carrie1/ecommerce-data). The file is `Online Retail.xlsx`.
  * **Convert to CSV:** Open the `.xlsx` file in a spreadsheet editor (Excel, Google Sheets) and save the first tab as `online_retail.csv`.
  * **Upload to DBFS/Volumes:**
      * In your Databricks Workspace, go to **"Catalog"** (or "Data").
      * Click on **"Upload to Volume"**.
      * Select your cluster.
      * **Destination path:** Create a path for your raw data. `/Volumes/main/default/ecommerce_data/raw/` is recommended if the `Volumes` feature is available. Otherwise, use `dbfs:/FileStore/datasets/ecommerce_data/raw/`.
      * Drag and drop your `online_retail.csv` file or click to select it, then finalize the upload.

### 3\. Notebook Importation

  * **Download the Notebooks:** Clone this repository to your local machine or download the `.py` files from the `notebooks/` folder.
  * **Import into Databricks:**
      * In your Databricks Workspace, go to **"Workspace"**.
      * Right-click on your user folder (or "Shared") and select **"Import"**.
      * Choose **"File"** and upload each `.py` file (e.g., `01_bronze_layer_ingestion.py`, `02_silver_layer_transformation.py`, `03_gold_layer_product.py`). Make sure **"Import as"** is set to **"Notebook"**.

### 4\. Pipeline Execution

  * **Attach to Cluster:** Open each notebook and ensure it's attached to the cluster you created.
  * **Execute in Order:** Run the notebooks in the following sequence:
    1.  `01_bronze_layer_ingestion.py`
    2.  `02_silver_layer_transformation.py`
    3.  `03_gold_layer_product.py`

Monitor each notebook's execution and check the outputs to ensure the pipeline is running correctly. The Delta tables will be created under `dbfs:/user/hive/warehouse/ecommerce_data/` (or a similar path) and can be queried via SQL or PySpark.

-----

## ðŸ“ˆ Visualization and Analysis

After pipeline execution, the tables in the Gold layer (`sales_by_product`, `daily_sales`, `top_customers`) will be ready for analysis. You can use Databricks' built-in visualization capabilities directly on the tables or export the data to external BI tools (Power BI, Tableau, Looker) to create interactive dashboards.

Example query in Databricks:

```sql
SELECT description, total_revenue
FROM sales_by_product
ORDER BY total_revenue DESC
LIMIT 20;
```

-----

## ðŸš€ Next Steps and Improvements

This project can be expanded with the following functionalities:

  * **Streaming Simulation with Auto Loader:** Implement Auto Loader in the Bronze layer notebook to simulate continuous ingestion of new sales files.
  * **Data Quality Validation:** Add more robust data quality checks in the Silver layer (e.g., consistency validations, anomaly detection).
  * **Performance Optimization:** Explore techniques like Delta table partitioning, Z-ordering, and compaction to optimize query performance.
  * **BI Tool Integration:** Connect Gold layer tables directly to BI tools for real-time sales dashboards.
  * **Job Orchestration:** Utilize Databricks Workflows (Jobs) to schedule and orchestrate the pipeline execution automatically.

-----

**Author:** Guilherme Noronha Mello -> github.com/guinnoronha
**Date:** June 2025

-----